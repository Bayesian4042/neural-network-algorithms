# Introduction
- BERT is a new language representation model, stands for Bidirection encoder representation transformer.

- Language model pre-training has shown to be effective to improving various natural language processing tasks.

- There are two existing strategies for applying to down-stream task:
    - feature based
    - fientuning 

There is long history of pretraining general language representation
1. Unsupervised feature based approach
2. Unsupervised finetuning based approach
    - pretrain from unlabelled text and fine-tune for a supervised downstream task. The advantage of this approach, is few parameters needs to be learn from scratch. 